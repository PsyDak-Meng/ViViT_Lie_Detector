<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision Project</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 40px 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }
        h1 {
            font-family: 'Montserrat', sans-serif;
            font-weight: 700;
            font-size: 3em;
            margin: 0;
        }
        section {
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin: 20px;
        }
        iframe {
            width: 100%;
            height: 400px;
            border: none;
            border-radius: 8px;
        }
        img {
            max-width: 80%;
            height: auto;
            display: block;
            margin: 0 auto;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        h1, h3,  h5 {
            color: #ffffff;
        }
        h2 {
            color: #1b1919b3;
        }
        p {
            color: #1c1a1a;
            line-height: 1.6;
        }
        .video-container {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
        }
        .video-container video {
            max-width: 48%;
            margin-bottom: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            display: inline-block;
        }
        .column {
            float: left;
            width: 49%;
            padding: 5px;
            }

            /* Clear floats after image containers */
            .row::after {
            content: "";
            clear: both;
            display: table;
            }
    </style>
</head>
<body>
    <header>
        <h1>ViViT Vision Transformer Video Lie Detector</h1>
        <h3>CS6476 Computer Vision Project</h3>
        <h5>Members: Cheting Meng, Vikram Sagar, Farhan Khan, Yiheng Mao</h5>
        <section>
            <ul class="icons alt">
                <li><a href="https://www.linkedin.com/in/Che-Ting-Meng1215" class="icon brands alt fa-linkedin" target="_blank" rel="noopener noreferrer"><span class="label">LinkedIn</span></a></li>
                <li><a href="https://github.com/PsyDak-Meng/ViViT_Lie_Detector" class="icon brands alt fa-github" target="_blank" rel="noopener noreferrer"><span class="label">GitHub</span></a></li>
            </ul>
        </section>
    </header>

    <section id="introduction">
        <h2>Introduction</h2>
        <!-- <p><b>Project Significance</b></p> -->
        <p>The main purpose of the project is to classify from video data whether an individual is lying and to extract key frames from the video and analyze the extracted expression to achieve algorithmic explainability. Detecting when a person is lying through a rigorously-tested AI model can play a significant role in judicial and legal settings, where an individual’s demeanor and facial expressions can indicate whether or not they are telling the truth. 
            <br>From an objective standpoint, the AI is highly unlikely to discriminate or bias, but rather classify whether or not the defendant (or accuser) is lying when under question. Regarding a criminal case, it is immediately apparent the advantages of a lie detector in determining the innocence or guiltiness of the accused in a court of law. Theoretically, the lie detector AI can play a considerable role in any political, economic, or medical institution, including holding a presidential candidate accountable to the information they may claim in a debate or the credibility of any doctors when under question of their practice. 
        </p> 
        <br>
        <p>
            <a href="https://docs.google.com/document/d/1P7fegZg60AUPF6zrQvOQy1gjKOaAqY5PR2XE-3iDXzM/edit">See full report</a>
            </p>
    </section>


    <section id="application_pesentation">
        <h2>Application Pesentation</h2>
        <div class="video-container">
            <iframe src="https://www.youtube.com/embed/yourlocalvideourl" allowfullscreen></iframe>
        </div>
        <!-- <p>This section displays a local video alongside the online video.</p> -->
    </section>

    <section id="interpret">
        <h2>AI Interpretability</h2>
        <div class="video-container">
            <div class="row">
                <div class="column">
                    <center>
                    <video controls>
                        <source src="web_data\BF001_1PT.wmv" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p>Example video input</p>
                    </center>
                    <section  style="outline: 2px dashed rgb(54, 54, 110)">
                    <p><b>Prediction output:</b><br>
                        The person is predicted to be truthful.<br>
                        The model has 61.77% confidence, and the prediction is correct!<br>
                        The predicted label is tensor([[1.]], the true label is tensor([1.]).<br><br>

                        Model attention layers: Attention(<br>
                        (to_qkv): Linear(in_features=192, out_features=576, bias=False)<br>
                        (to_out): Sequential(<br>
                            (0): Linear(in_features=192, out_features=192, bias=True)<br>
                            (1): Dropout(p=0.0, inplace=False)<br>
                        )
                        )<br>
                        Attention weights shape:  (105,)<br><br>

                        Top 4 significant attention frames: [50, 62, 82, 74] <br>
                        Corresponding weights:  [0.00953582, 0.00953374, 0.0095327,  0.00953227]</p>
                    </section>
                </div>
                <div class="column">
                    <img src="web_data/output.jpg" alt="Computer vision project image">
                </div>
            </div>
        </div>
        <div>
            
        </div>
    </section>

    <section id="framework">
        <h2>Framework</h2>
        <div class="row">
            <div class="column">
                <img src="web_data/framework.jpg" alt="FRAMEWORK" id="framework_img">
            </div>
            <div class="column">
                <img src="web_data/vivit.png" alt="vivit" >
                <p><center>ViViT model framework</center></p>
            </div>
            
        </div>
        <p><b>Methodology:</b></p>
        <p>The ViViT model first extracts spatial-temporal tokens from the input video by embedding, which are then encoded by a series of transformer layers. The model then uses several variations of attention to factorize the large spatial and temporal dimensions of the video. Specifically, it uses a factorized encoder layer, a factorized self-attention layer, or a factorized dot-product layer. In our case, we primarily use a factorized self-attention layer. This allows us to effectively train and regularize the model when the datasets are not large enough.
            <br><br><b>Problem with sampling key frames in ViViT:</b>
            <br>While ViViT achieved good results on video classification, however, the two methods used to sample frames --1) uniform sampling: Uniformly sampling picture frames from the video with fixed time span and dividing into image patches; 2) Tubelet embedding: selecting fixed time span of patches in frames into embeddings, are both arguably flawed for capturing transient frames such as micro expressions. Uniform sampling has no guarantee of capturing the exact high-relevance frames, and Tubelet embedding can result in low attention weights of tubes containing high-relevance frames because of additional low-relevance expressions. As a result, we would like to improve the sampling process to obtain more relevant key frames.
            <br><br><b>Improving key frame sampling through facial action unit tracking:</b>
            <br>Instead of sampling key frames by uniform sampling and Tubelet embedding, we use a technique called Facial Action Unit (FAU) tracking. A FAU is a term used in facial expression analysis and the study of human facial movements. It was introduced by psychologists Paul Ekman and Wallace V. Friesen as part of their Facial Action Coding System (FACS). We can calculate FAU intensity using facial landmarks. Higher FAU intensity means higher likelihood of micro expressions. In the context of lie detection, micro expressions can be a good indicator of whether the person is lying. Thus, by sampling key frames with high intensity, we can capture frames more relevant to the prediction.
            <img src="web_data/FAU_detection.png">
            <br>Here is how FAU based frame sampling works. At first, "get_frontal_face_detector" function from the dlib library was utilized to detect faces in the frames. Then "shape_predictor_68_face_landmarks" was applied as a predictor to identify 68 points along the contour of the face (Figure 3). Using these landmarks, 10 facial action units such as inner brow raiser, outer brow raiser, brow lowerer, cheek raiser, eye lid tightener, nose wrinkler lip corner puller, lip corner depressor, and lip part were calculated to track facial actions.A signal was obtained for each FAU. The "find_peaks_cwt" function from the scipy.signal library was used to detect peaks.
            <br>From 10 FAUs, we got 10 sets of peaks. Next, we identified the unique peaks from all 10 sets. Finally, we filtered the unique peaks by selecting one peaks for 5 consecutive frames.
            <div class="row">
                <div class="column">
                    <img src="web_data/Per_FAU_unique_peaks.png" alt="FRAMEWORK" id="Per_FAU_unique_peaks">
                    <p><center>Peak detection for each FAU</center></p>
                </div>
                <div class="column">
                    <img src="web_data/All_unique_peaks.png" alt="All_unique_peaks" >
                    <p><center>Unique frames combined from all FAU signals (with filtering)</center></p>
                </div>
            </div>
        </p>
        <p><b>Framework:</b></p> 
        <p>The video is processed through FAU, then the high intensity frames are extracted for the ViViT model, each video sample is represented as a 4-dimensional array [frame_num, height, width,channel], where frame numbers are padded to the maximum length. By using the vision transformer architecture, we are allowing inputs of different temporal lengths, which makes the framework flexible. Lastly, the ViViT outputs a percentage prediction that predicts the veracity label of  a given video.
        <p><b>Result and Discussion:</b></p> 
        <p>The final accuracy of our model is around 59 percent. This is not an ideal result, but we have some theory on what the underlying issue is. First, since almost no body language is involved in all the videos, the only useful insight we can extract from each frame is the person’s expressions. And because expressions are mostly micro compared to pronounced ones like crying or laughing, the difference between each frame is relatively small. This means even though FAU tries to sample frames with most insights, the number of pixels in each frame that are useful is still very small. Useful pixels could be buried by the large number of non-insightful pixels in the frame. Second, our dataset of 321 videos is very small compared to what’s used in state-of-the-art models. If we were to artificially inflate the sample size using bootstrapping, there could be even more overfitting. It is rather difficult to properly train the weights when the sample size is very small compared to the number of parameters—the degree of freedom for parameters is just too large. In an ideal scenario, we would be fine tuning the model from some pre-trained weights. Unfortunately, ViViT is quite new and there aren’t pre-trained weights available.
        <p><b>Future Work:</b></p> 
        <p>We believe the issues discussed above likely contributed to the non-ideal performance of our mode. And we are confident that model accuracy will significantly improve. Specifically, if we can develop some algorithm to only capture pixels where micro-expressions are present and discard the rest “static” pixels, the model could extract more insight. And if we have the compute resources and dataset to train the model on a large set of videos (preferably more than 10k), we could have accuracy level comparable to that of commercial models. We feel those are out of the scope of this course, but we are excited to explore more in the future.
            
    </section>
</body>
</html>
